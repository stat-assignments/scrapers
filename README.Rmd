---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# scrapers

<!-- badges: start -->
[![R-CMD-check](https://github.com/stat-assignments/scrapers/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/stat-assignments/scrapers/actions/workflows/R-CMD-check.yaml)
[![Codecov test coverage](https://codecov.io/gh/stat-assignments/scrapers/graph/badge.svg)](https://app.codecov.io/gh/stat-assignments/scrapers)
<!-- badges: end -->

The goal of scrapers is to provide functionality for an easy update of data used throughout the assignments in `stat-assignments`


## Installation

You can install the development version of scrapers like so:

``` r
remotes::install_github("stat-assignments/scraper")
```

## Example

This is a basic example which shows you how to solve a common problem:

```{r example}
library(scrapers)
library(tidyverse, quietly = TRUE)
## basic example code

eq <- get_earthquakes(lubridate::today()-30, lubridate::today())
eq %>% ggplot(aes(x = longitude, y = latitude)) + geom_point()
```

## Usage


```{r}
usage <- data.frame(`function` = "get_earthquake", repo = "data-earthquakes", link="https://github.com/stat-assignments/data-earthquakes") 

usage <- usage |> 
  rbind(data.frame(
    `function` = "get_user_comments", 
    repo = "dplyr-reddit-poetry", 
    link="https://github.com/stat-assignments/dplyr-reddit-poetry") ) |>
  rbind(data.frame(
    `function` = "get_pets", 
    repo = "dplyr-reddit-poetry", 
    link="https://github.com/stat-assignments/dplyr-reddit-poetry") )

usage |> knitr::kable()
```
