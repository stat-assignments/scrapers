[{"path":"https://stat-assignments.github.io/scrapers/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 scrapers authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://stat-assignments.github.io/scrapers/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Heike Hofmann. Author, maintainer, copyright holder. Susan Vanderplas. Author.","code":""},{"path":"https://stat-assignments.github.io/scrapers/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hofmann H, Vanderplas S (2025). scrapers: Scrapers setting updating data stat-assigments. R package version 0.1.0, https://github.com/stat-assignments/scrapers.","code":"@Manual{,   title = {scrapers: Scrapers for setting up and updating data in stat-assigments},   author = {Heike Hofmann and Susan Vanderplas},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/stat-assignments/scrapers}, }"},{"path":"https://stat-assignments.github.io/scrapers/index.html","id":"scrapers","dir":"","previous_headings":"","what":"Scrapers for setting up and updating data in stat-assigments","title":"Scrapers for setting up and updating data in stat-assigments","text":"goal scrapers provide functionality easy update data used throughout assignments stat-assignments","code":""},{"path":"https://stat-assignments.github.io/scrapers/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Scrapers for setting up and updating data in stat-assigments","text":"can install development version scrapers like :","code":"remotes::install_github(\"stat-assignments/scraper\")"},{"path":"https://stat-assignments.github.io/scrapers/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Scrapers for setting up and updating data in stat-assigments","text":"basic example shows solve common problem:","code":"library(scrapers) library(tidyverse, quietly = TRUE) #> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── #> ✔ dplyr     1.1.4     ✔ readr     2.1.5 #> ✔ forcats   1.0.0     ✔ stringr   1.5.1 #> ✔ ggplot2   3.5.2     ✔ tibble    3.2.1 #> ✔ lubridate 1.9.4     ✔ tidyr     1.3.1 #> ✔ purrr     1.0.4      #> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #> ✖ dplyr::filter() masks stats::filter() #> ✖ dplyr::lag()    masks stats::lag() #> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors ## basic example code  eq <- get_earthquakes(lubridate::today()-30, lubridate::today()) eq %>% ggplot(aes(x = longitude, y = latitude)) + geom_point()"},{"path":"https://stat-assignments.github.io/scrapers/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Scrapers for setting up and updating data in stat-assigments","text":"","code":"usage <- data.frame(`function` = \"get_earthquake\", repo = \"data-earthquakes\", link=\"https://github.com/stat-assignments/data-earthquakes\")   usage <- usage |> rbind(data.frame(   `function` = \"get_earthquake\",    repo = \"data-earthquakes\",    link=\"https://github.com/stat-assignments/data-earthquakes\") )  usage |> knitr::kable()"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_earthquakes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get earthquake information from USGS — get_earthquakes","title":"Get earthquake information from USGS — get_earthquakes","text":"Downloads records earth quakes USGS Earthquake Catalog","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_earthquakes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get earthquake information from USGS — get_earthquakes","text":"","code":"get_earthquakes(   start_time = today() - 30,   end_time = NULL,   min_magnitude = NULL,   base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\" )"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_earthquakes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get earthquake information from USGS — get_earthquakes","text":"start_time character string specifying start date records form 'yyyy-mm-dd' end_time character string specifying end date records form 'yyyy-mm-dd' min_magnitude numeric quantity specifying minimum magnitude considered record base_url base url GET request","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_earthquakes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get earthquake information from USGS — get_earthquakes","text":"data frame earth quake records. Detailed information variables can found part [USGS ComCat Documentation](https://earthquake.usgs.gov/data/comcat/index.php). default, data last 30 days requested archive. similar data USGS provides `Earthquakes` past 30 days spreadsheet/csv feed https://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_earthquakes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get earthquake information from USGS — get_earthquakes","text":"","code":"# example code library(lubridate) #>  #> Attaching package: ‘lubridate’ #> The following objects are masked from ‘package:base’: #>  #>     date, intersect, setdiff, union todays <- get_earthquakes(start_time = today()-1, end_time = today()) # all documented earthquakes with a magnitude of at least 9 on the Richter scale nines <-  get_earthquakes(start_time = \"1800-01-01\", end_time = today(), min_magnitude=9)"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_petfinder_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get adoptable pets from Petfinder — get_petfinder_data","title":"Get adoptable pets from Petfinder — get_petfinder_data","text":"Downloads records adoptable pets. need key secret [PetFinder API](https://www.petfinder.com/developers/).","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_petfinder_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get adoptable pets from Petfinder — get_petfinder_data","text":"","code":"get_petfinder_data(   token = NULL,   endpoint = \"animals\",   params = list(type = \"cat\", location = \"68528\"),   page = 1,   base_url = \"https://api.petfinder.com/v2/\" )  petfinder_get_token(   key = get_api_key(\"PETFINDER_API\"),   secret = get_api_secret(\"PETFINDER_API\"),   base_url = \"https://api.petfinder.com/v2\" )"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_petfinder_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get adoptable pets from Petfinder — get_petfinder_data","text":"token authentication token. Can passed result calling `petfinder_get_token`. absent, one attempt made creating new token. endpoint character value - endpoint Petfinder API used? API v2 endpoints `animals` `organizations`. page positive integer value, defaults one. multi-page results, way pick previous result. base_url characterstring API key character API key, specified, loaded environment secret character API secret, specified, loaded environment param (list ) parameters pass endpoint. incorporated query form `name='value'&...`","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_petfinder_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get adoptable pets from Petfinder — get_petfinder_data","text":"list (pages ) records returned Petfinder API (JSON-style list). element list represents one page results. page results organized form 'animals' (records 20 animals) 'pagination'. Records returned increasing page order.","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_petfinder_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get adoptable pets from Petfinder — get_petfinder_data","text":"","code":"# example code # Make sure to get a token before your first use: petf_token <- petfinder_get_token() #> Searching for key <PETFINDER_API_KEY> ...  #> Error in get_api_key(\"PETFINDER_API\"):  #> No API key found, please supply with `key` argument or store in environment  cats <- get_petfinder_data(token=petf_token) #> Error: object 'petf_token' not found length(cats) #> Error: object 'cats' not found # each element is a list of animals and pagination information: cats[1:5] |> str(max.level = 2) #> Error: object 'cats' not found cats[[21]]$pagination[5] # the last element in the pagination vector contains the link to the next set of records, if available #> Error: object 'cats' not found cats2 <- get_petfinder_data(token=petf_token, page = 22) #> Error: object 'petf_token' not found # not a complete list anymore: cats2[[21]]$animals #> Error: object 'cats2' not found animals <- cats |> purrr::map(.f = function(x) x$animals) #> Error: object 'cats' not found animals[21+(1:21)] <- cats2 |> purrr::map(.f = function(x) x$animals) #> Error: object 'cats2' not found animals <- animals |> purrr::compact() # get rid of empty elements #> Error: object 'animals' not found length(animals) #> Error: object 'animals' not found # flatten the structure once: animal_record_list <- animals |> purrr::flatten() #> Error: object 'animals' not found jsonlite::write_json(animal_record_list, path = \"cats.json\") #> Error: object 'animal_record_list' not found # Authenticate with the petfinder API and get a token token <- petfinder_get_token() #> Searching for key <PETFINDER_API_KEY> ...  #> Error in get_api_key(\"PETFINDER_API\"):  #> No API key found, please supply with `key` argument or store in environment"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_user_comments.html","id":null,"dir":"Reference","previous_headings":"","what":"Get user content from reddit — get_user_comments","title":"Get user content from reddit — get_user_comments","text":"Download individual user's content reddit","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_user_comments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get user content from reddit — get_user_comments","text":"","code":"get_user_comments(   user_name,   limit = 10L,   after = NULL,   base_url = \"https://www.reddit.com/\" )"},{"path":"https://stat-assignments.github.io/scrapers/reference/get_user_comments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get user content from reddit — get_user_comments","text":"user_name character string individual reddit user limit integer value 1 100 character string specifying location list user comments. specified, comments location returned (found). base_url base url GET request","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_user_comments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get user content from reddit — get_user_comments","text":"list containing named items: `data`, ``, `request`. `data` data frame reddit comments user. `` character value hashed location last comment included returned items. value can used starting point next batch responses. `request` exact string results returned.","code":""},{"path":"https://stat-assignments.github.io/scrapers/reference/get_user_comments.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get user content from reddit — get_user_comments","text":"","code":"# example code sprog2 <- get_user_comments(user_name=\"poem_for_your_sprog\", limit=2) #> Error in get_user_comments(user_name = \"poem_for_your_sprog\", limit = 2): Error in req_perform(req) : HTTP 403 Forbidden. sprog_next2 <- get_user_comments(user_name=\"poem_for_your_sprog\", limit=2, after=sprog2$after) #> Error: object 'sprog2' not found sprog4 <- get_user_comments(user_name=\"poem_for_your_sprog\", limit=4) #> Error in get_user_comments(user_name = \"poem_for_your_sprog\", limit = 4): Error in req_perform(req) : HTTP 403 Forbidden."}]
